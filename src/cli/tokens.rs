use std::path::PathBuf;
use std::collections::HashSet;

use clap::Parser;
use colorful::Colorful;

use crate::prelude::*;

#[derive(Parser)]
pub enum TokensCLI {
    /// Create new tokens database.
    Create,

    /// Insert all tokens from the documents database.
    Update {
        #[arg(long, short)]
        /// Path to the documents database.
        documents: PathBuf,

        #[arg(long, short)]
        /// Convert content of the documents to lowercase.
        lowercase: bool,

        #[arg(long, short)]
        /// Strip punctuation from the documents.
        strip_punctuation: bool,

        #[arg(long, short)]
        /// Save whitespace characters as tokens.
        whitespace_tokens: bool
    }
}

impl TokensCLI {
    #[inline]
    pub fn execute(self, database: PathBuf, cache_size: i64) -> anyhow::Result<()> {
        match self {
            Self::Create => {
                let database = database.canonicalize().unwrap_or(database);

                println!("â³ Creating tokens database in {database:?}...");

                match TokensDatabase::open(&database, cache_size) {
                    Ok(_) => {
                        println!("{}", "ðŸš€ Database created".green());
                        println!("ðŸ“– {} {} command will create new database automatically if needed", "Note:".blue(), "`tokens update`".yellow());
                    }

                    Err(err) => eprintln!("{}", format!("ðŸ§¯ Failed to create database: {err}").red())
                }
            }

            Self::Update { documents, lowercase, strip_punctuation, whitespace_tokens } => {
                let tokens = database.canonicalize().unwrap_or(database);
                let documents = documents.canonicalize().unwrap_or(documents);

                println!("â³ Opening tokens database in {tokens:?}...");

                let mut tokens = match TokensDatabase::open(&tokens, cache_size) {
                    Ok(tokens) => tokens,
                    Err(err) => {
                        eprintln!("{}", format!("ðŸ§¯ Failed to open tokens database: {err}").red());

                        return Ok(());
                    }
                };

                println!("â³ Opening documents database in {documents:?}...");

                let documents = match DocumentsDatabase::open(&documents, cache_size) {
                    Ok(documents) => documents,
                    Err(err) => {
                        eprintln!("{}", format!("ðŸ§¯ Failed to open documents database: {err}").red());

                        return Ok(());
                    }
                };

                let parser = DocumentsParser::new(lowercase, strip_punctuation, whitespace_tokens);

                println!("â³ Updating tokens database...");

                let mut inserted_tokens = HashSet::new();

                let transaction = match tokens.insert_tokens() {
                    Ok(transaction) => transaction,
                    Err(err) => {
                        eprintln!("{}", format!("ðŸ§¯ Failed to open tokens insertion transaction: {err}").red());

                        return Ok(());
                    }
                };

                documents.for_each(|document| {
                    for token in parser.read_document(document) {
                        // We insert tokens even if they were already inserted
                        // because tokens database calculates occurences of tokens
                        // within all text corpuses.
                        transaction.insert_token(&token)?;

                        inserted_tokens.insert(token);
                    }

                    Ok(())
                })?;

                match transaction.commit() {
                    Ok(()) => println!("âœ… Updated {} tokens", inserted_tokens.len().to_string().yellow()),
                    Err(err) => eprintln!("{}", format!("ðŸ§¯ Failed to commit tokens insertion transaction: {err}").red())
                }
            }
        }

        Ok(())
    }
}
